翻译学习[Fast sort on CPUs and GPUs: a case for bandwidth oblivious SIMD sort.](https://www.researchgate.net/publication/221213255_Fast_sort_on_CPUs_and_GPUs_a_case_for_bandwidth_oblivious_SIMD_sort)  
谷歌翻译很好用
## 3 在现代处理器上排序算法的表现

尽管不同排序算法的理论计算复杂度会对性能产生一定影响，但它们使用不同硬件功能的效率通常在性能方面具有决定性作用。 随着内存大小的增加，内存数据库现在很常见，因此I/O并不是主要限制。现在，性能限制已转移到体系结构中可用的计算和主内存资源。现在，我们说明一些最近的体系结构改进及其在数据库操作环境中对排序性能的影响。
## 3.1 线程层面和数据层面的并行
现代处理器通过（a）添加更多内核以利用线程级并行性，以及（b）添加SIMD单元以利用数据级并行性来提高了计算能力。 已经针对不同的多核体系结构提出了许多排序算法的线程并行实现。 自然地，诸如合并和快速分类之类的许多分类涉及并行地组合或分割不同的数据块。 其他排序，例如基数排序，可以通过阻止输入数据并使用全局直方图更新来协调块之间进行并行化（请参阅Blelloch [6]）。 这些已显示出可以在CPU和GPU架构上很好地扩展。  
在某些排序算法中，数据级并行性很难利用。 为了有效地执行SIMD，必须将要操作的数据连续放置在内存中。 在没有连续访问的情况下，需要对内存的收集/分散1，这在大多数体系结构上都很慢。 例如，基数排序通常涉及到常见的直方图更新和不规则的内存访问以重新排列数据。 这两种方法本质上都不适合SIMD执行。 已经提出了对SIMD更友好的基数排序变体[27]； 但是它们需要更多指令才能执行。 另一方面，`合并排序可以使用SIMD友好的排序网络`[9]。 随着SIMD宽度的增加，对SIMD友好的排序变得越来越有效。  
## 3.2 内存带宽
排序从根本上涉及重新安排驻留在内存中的数据，因此通常会占用大量内存。内存带宽的提高没有现代处理器的计算能力大，并且未来每核的带宽有望进一步下降[21]。要求高内存带宽的算法很可能会成为带宽限制，因此就内核数和SIMD宽度而言停止扩展。为了弥合应用程序带宽需求和可用带宽之间的差距，架构以CPU上的缓存层次结构和GPU上的共享内存的形式引入了片上本地存储。如果正在处理的数据集适合该存储，则不使用主内存带宽；否则从主存储器读取和写入数据。重新设计了排序算法以使用此类存储区域。就带宽使用而言，可以使合并排序非常有效，通过多路合并实现[11]仅需要两次读取和两次写入数据，即可限制工作集以适合高速缓存。`基数排序不是带宽友好的`。如果输入数据远大于缓存大小，则必须在每次通过中读写每个输入键。  
## 3.3 
## 4 基数排序
在本节中，我们将在4.1节中描述基本的基数排序算法及其并行实现。 在第4.2节中，我们然后提出了一种基于缓冲区的方案，以使基数排序体系结构友好，并且还描述了基于流拆分(stream splits)的先前本地排序方法。 然后，我们将在第4.3和4.4节中描述基数排序的最佳CPU和GPU实现，并提供一个性能模型来分析其性能。  
## 4.1 基础平行算法
